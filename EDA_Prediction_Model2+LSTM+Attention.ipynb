{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install numpy pandas scikit-learn tensorflow pyEDFlib biosppy\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score, recall_score\n",
        "import biosppy.signals.eda as eda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Attention\n"
      ],
      "metadata": {
        "id": "R8jhcnpfmho8",
        "outputId": "452e8599-82f6-4998-e05d-0656fc1fa670",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Collecting pyEDFlib\n",
            "  Downloading pyEDFlib-0.1.38-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting biosppy\n",
            "  Downloading biosppy-2.2.2-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: bidict in /usr/local/lib/python3.10/dist-packages (from biosppy) (0.23.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from biosppy) (3.7.1)\n",
            "Collecting shortuuid (from biosppy)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from biosppy) (4.10.0.84)\n",
            "Collecting pywavelets (from biosppy)\n",
            "  Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting mock (from biosppy)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Downloading pyEDFlib-0.1.38-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biosppy-2.2.2-py2.py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: shortuuid, pywavelets, pyEDFlib, mock, biosppy\n",
            "Successfully installed biosppy-2.2.2 mock-5.1.0 pyEDFlib-0.1.38 pywavelets-1.7.0 shortuuid-1.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "def load_and_extract_eda(file_path):\n",
        "    df = pd.read_csv(file_path, skiprows=3, header=None)  # Skip the first 3 rows of metadata\n",
        "    eda_signal = df[0].values\n",
        "\n",
        "    # Normalize the signal to be between 0 and 1\n",
        "    eda_signal = (eda_signal - np.min(eda_signal)) / (np.max(eda_signal) - np.min(eda_signal))\n",
        "\n",
        "    # Extract phasic and tonic components\n",
        "    sampling_rate = 4.0  # E4 Empatica sampling rate is 4Hz\n",
        "\n",
        "    # Bandpass filter parameters for phasic component\n",
        "    lowcut = 0.05  # Hz\n",
        "    highcut = 0.5  # Hz\n",
        "    nyquist_freq = 0.5 * sampling_rate\n",
        "\n",
        "    # Normalize frequencies\n",
        "    low = lowcut / nyquist_freq\n",
        "    high = highcut / nyquist_freq\n",
        "\n",
        "    # Butterworth bandpass filter\n",
        "    b, a = butter(2, [low, high], btype='bandpass')\n",
        "    phasic = filtfilt(b, a, eda_signal)\n",
        "\n",
        "    # Lowpass filter for tonic component\n",
        "    lowcut_tonic = 0.05 / nyquist_freq\n",
        "    b_tonic, a_tonic = butter(2, lowcut_tonic, btype='low')\n",
        "    tonic = filtfilt(b_tonic, a_tonic, eda_signal)\n",
        "\n",
        "    return phasic, tonic\n",
        "\n",
        "\n",
        "# Function to load all data for a participant\n",
        "def load_all_data(participant_num, num_files=10):\n",
        "    phasic_list = []\n",
        "    tonic_list = []\n",
        "    for file_num in range(1, num_files + 1):\n",
        "        file_path = f\"EDA_{participant_num}_{file_num}.csv\"\n",
        "        phasic, tonic = load_and_extract_eda(file_path)\n",
        "        phasic_list.append(phasic)\n",
        "        tonic_list.append(tonic)\n",
        "\n",
        "    return np.concatenate(phasic_list), np.concatenate(tonic_list)\n",
        "\n",
        "# Load data for all participants\n",
        "train_data_phasic, train_data_tonic = [], []\n",
        "for participant_num in range(1, 5):\n",
        "    phasic, tonic = load_all_data(participant_num)\n",
        "    train_data_phasic.append(phasic)\n",
        "    train_data_tonic.append(tonic)\n",
        "\n",
        "train_data_phasic = np.concatenate(train_data_phasic)\n",
        "train_data_tonic = np.concatenate(train_data_tonic)\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_phasic = train_data_phasic[:int(len(train_data_phasic) * 0.7)]\n",
        "val_phasic = train_data_phasic[int(len(train_data_phasic) * 0.7):int(len(train_data_phasic) * 0.85)]\n",
        "test_phasic = train_data_phasic[int(len(train_data_phasic) * 0.85):]\n",
        "\n",
        "train_tonic = train_data_tonic[:int(len(train_data_tonic) * 0.7)]\n",
        "val_tonic = train_data_tonic[int(len(train_data_tonic) * 0.7):int(len(train_data_tonic) * 0.85)]\n",
        "test_tonic = train_data_tonic[int(len(train_data_tonic) * 0.85):]\n"
      ],
      "metadata": {
        "id": "iNPa6AkJmkNz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization of Data\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F0ZXuMDDnGsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_phasic = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_tonic = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "train_phasic = scaler_phasic.fit_transform(train_phasic.reshape(-1, 1)).flatten()\n",
        "val_phasic = scaler_phasic.transform(val_phasic.reshape(-1, 1)).flatten()\n",
        "test_phasic = scaler_phasic.transform(test_phasic.reshape(-1, 1)).flatten()\n",
        "\n",
        "train_tonic = scaler_tonic.fit_transform(train_tonic.reshape(-1, 1)).flatten()\n",
        "val_tonic = scaler_tonic.transform(val_tonic.reshape(-1, 1)).flatten()\n",
        "test_tonic = scaler_tonic.transform(test_tonic.reshape(-1, 1)).flatten()\n"
      ],
      "metadata": {
        "id": "jhXaqaTfm-TB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Prepration"
      ],
      "metadata": {
        "id": "CoC1DbR9nUOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, time_steps=20):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps):\n",
        "        X.append(data[i:i + time_steps])\n",
        "        y.append(data[i + time_steps])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create sequences\n",
        "time_steps = 20\n",
        "X_train_phasic, y_train_phasic = create_sequences(train_phasic, time_steps)\n",
        "X_val_phasic, y_val_phasic = create_sequences(val_phasic, time_steps)\n",
        "X_test_phasic, y_test_phasic = create_sequences(test_phasic, time_steps)\n",
        "\n",
        "X_train_tonic, y_train_tonic = create_sequences(train_tonic, time_steps)\n",
        "X_val_tonic, y_val_tonic = create_sequences(val_tonic, time_steps)\n",
        "X_test_tonic, y_test_tonic = create_sequences(test_tonic, time_steps)\n"
      ],
      "metadata": {
        "id": "djbtu2osnBIW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Attention-Based LSTM Model"
      ],
      "metadata": {
        "id": "45CDpH_8ngVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_attention_lstm_model(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    lstm_out = LSTM(64, return_sequences=True)(input_layer)\n",
        "    attention = Attention()([lstm_out, lstm_out])\n",
        "    lstm_out2 = LSTM(32)(attention)\n",
        "    output_layer = Dense(1)(lstm_out2)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
        "    return model\n",
        "\n",
        "input_shape = (time_steps, 1)\n",
        "model_phasic = create_attention_lstm_model(input_shape)\n",
        "model_tonic = create_attention_lstm_model(input_shape)\n"
      ],
      "metadata": {
        "id": "EnsIRPMYnXAB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_phasic = model_phasic.fit(X_train_phasic, y_train_phasic, validation_data=(X_val_phasic, y_val_phasic), epochs=10, batch_size=32)\n",
        "history_tonic = model_tonic.fit(X_train_tonic, y_train_tonic, validation_data=(X_val_tonic, y_val_tonic), epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "gbFlRL4wnhhX",
        "outputId": "25421660-18e2-41ad-94ee-c3eec5012510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 24ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0024 - val_mse: 0.0024\n",
            "Epoch 2/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 24ms/step - loss: 7.8926e-04 - mse: 7.8926e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 3/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 25ms/step - loss: 4.7145e-04 - mse: 4.7145e-04 - val_loss: 2.1734e-04 - val_mse: 2.1734e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 24ms/step - loss: 9.4340e-05 - mse: 9.4340e-05 - val_loss: 9.1215e-05 - val_mse: 9.1215e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 25ms/step - loss: 5.8880e-05 - mse: 5.8880e-05 - val_loss: 6.9838e-05 - val_mse: 6.9838e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 24ms/step - loss: 3.8685e-05 - mse: 3.8685e-05 - val_loss: 6.8705e-05 - val_mse: 6.8705e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 24ms/step - loss: 3.3557e-05 - mse: 3.3557e-05 - val_loss: 5.8580e-05 - val_mse: 5.8580e-05\n",
            "Epoch 8/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 24ms/step - loss: 3.1751e-05 - mse: 3.1751e-05 - val_loss: 4.9853e-05 - val_mse: 4.9853e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 24ms/step - loss: 2.7618e-05 - mse: 2.7618e-05 - val_loss: 5.2933e-05 - val_mse: 5.2933e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 24ms/step - loss: 2.8463e-05 - mse: 2.8463e-05 - val_loss: 2.3061e-04 - val_mse: 2.3061e-04\n",
            "Epoch 1/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 25ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 9.3096e-04 - val_mse: 9.3096e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 24ms/step - loss: 8.7961e-04 - mse: 8.7961e-04 - val_loss: 2.8572e-04 - val_mse: 2.8572e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 25ms/step - loss: 2.9153e-04 - mse: 2.9153e-04 - val_loss: 4.2669e-04 - val_mse: 4.2669e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 24ms/step - loss: 3.0118e-04 - mse: 3.0118e-04 - val_loss: 1.7376e-04 - val_mse: 1.7376e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 25ms/step - loss: 1.9110e-04 - mse: 1.9110e-04 - val_loss: 2.0068e-04 - val_mse: 2.0068e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 25ms/step - loss: 2.4098e-04 - mse: 2.4098e-04 - val_loss: 2.8551e-04 - val_mse: 2.8551e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 25ms/step - loss: 2.5475e-04 - mse: 2.5475e-04 - val_loss: 2.5209e-04 - val_mse: 2.5209e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 25ms/step - loss: 1.7510e-04 - mse: 1.7510e-04 - val_loss: 2.3999e-04 - val_mse: 2.3999e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 25ms/step - loss: 2.0220e-04 - mse: 2.0220e-04 - val_loss: 1.6764e-04 - val_mse: 1.6764e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 25ms/step - loss: 1.4650e-04 - mse: 1.4650e-04 - val_loss: 1.7353e-04 - val_mse: 1.7353e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X, y, scaler):\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
        "    y_rescaled = scaler.inverse_transform(y.reshape(-1, 1))\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_rescaled, y_pred_rescaled))\n",
        "    accuracy = accuracy_score(np.round(y_rescaled), np.round(y_pred_rescaled))\n",
        "    f1 = f1_score(np.round(y_rescaled), np.round(y_pred_rescaled), average='macro')\n",
        "    sensitivity = recall_score(np.round(y_rescaled), np.round(y_pred_rescaled), average='macro')\n",
        "    auroc = roc_auc_score(np.round(y_rescaled), y_pred_rescaled)\n",
        "\n",
        "    return {\"RMSE\": rmse, \"Accuracy\": accuracy, \"F1 Score\": f1, \"Sensitivity\": sensitivity, \"AUROC\": auroc}\n",
        "\n",
        "metrics_phasic = evaluate_model(model_phasic, X_test_phasic, y_test_phasic, scaler_phasic)\n",
        "metrics_tonic = evaluate_model(model_tonic, X_test_tonic, y_test_tonic, scaler_tonic)\n",
        "print(\"Phasic Component Metrics:\", metrics_phasic)\n",
        "print(\"Tonic Component Metrics:\", metrics_tonic)\n"
      ],
      "metadata": {
        "id": "Y5XJrE3v162h",
        "outputId": "b107c24e-83fa-4505-a5f9-de0ee0a87344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m477/477\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step\n",
            "\u001b[1m477/477\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step\n",
            "Phasic Component Metrics: {'RMSE': 0.010272080208788864, 'Accuracy': 0.9998031496062992, 'F1 Score': 0.4999507825573383, 'Sensitivity': 0.5, 'AUROC': 0.9999343702828641}\n",
            "Tonic Component Metrics: {'RMSE': 0.011359482356376852, 'Accuracy': 0.9955380577427821, 'F1 Score': 0.995388559926365, 'Sensitivity': 0.9953402447163515, 'AUROC': 0.9995822024471636}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load new EDA data file\n",
        "new_data_file_path = \"EDA_Testing.csv\"  # Replace with the actual path to your new data file\n",
        "\n",
        "# Extract phasic and tonic components from the new data\n",
        "new_phasic, new_tonic = load_and_extract_eda(new_data_file_path)\n",
        "\n",
        "# Normalize the new data using previously fitted scalers\n",
        "new_phasic_normalized = scaler_phasic.transform(new_phasic.reshape(-1, 1)).flatten()\n",
        "new_tonic_normalized = scaler_tonic.transform(new_tonic.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split the new data into 80% training input and 20% test\n",
        "split_index_phasic = int(len(new_phasic_normalized) * 0.8)\n",
        "split_index_tonic = int(len(new_tonic_normalized) * 0.8)\n",
        "\n",
        "X_new_phasic = new_phasic_normalized[:split_index_phasic]\n",
        "y_true_phasic = new_phasic_normalized[split_index_phasic:]\n",
        "\n",
        "X_new_tonic = new_tonic_normalized[:split_index_tonic]\n",
        "y_true_tonic = new_tonic_normalized[split_index_tonic:]\n"
      ],
      "metadata": {
        "id": "o2YgTvOl2uI5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences for the new phasic and tonic data\n",
        "X_eval_phasic, _ = create_sequences(X_new_phasic, time_steps=len(y_true_phasic))\n",
        "X_eval_tonic, _ = create_sequences(X_new_tonic, time_steps=len(y_true_tonic))\n"
      ],
      "metadata": {
        "id": "kTkP49iA2xf2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of sequences for prediction\n",
        "num_sequences = len(y_true_phasic) - time_steps + 1\n",
        "\n",
        "# Adjust X_eval_phasic and X_eval_tonic based on the new sequence length\n",
        "X_eval_phasic, _ = create_sequences(X_new_phasic, time_steps)\n",
        "X_eval_tonic, _ = create_sequences(X_new_tonic, time_steps)\n",
        "\n",
        "# Use only the necessary number of sequences that match the expected output\n",
        "X_eval_phasic = X_eval_phasic[-num_sequences:]\n",
        "X_eval_tonic = X_eval_tonic[-num_sequences:]\n"
      ],
      "metadata": {
        "id": "Zqtrr6hN3MjX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the remaining 20% using the trained model\n",
        "y_pred_phasic = model_phasic.predict(X_eval_phasic)\n",
        "y_pred_tonic = model_tonic.predict(X_eval_tonic)\n",
        "\n",
        "# Inverse the scaling to get back to original values\n",
        "y_pred_phasic = scaler_phasic.inverse_transform(y_pred_phasic)\n",
        "y_true_phasic = y_true_phasic[-len(y_pred_phasic):]  # Align true values with predicted length\n",
        "\n",
        "y_pred_tonic = scaler_tonic.inverse_transform(y_pred_tonic)\n",
        "y_true_tonic = y_true_tonic[-len(y_pred_tonic):]  # Align true values with predicted length\n"
      ],
      "metadata": {
        "id": "1K5E0-B13Nnm",
        "outputId": "0e9c373a-be8f-488a-e54f-1cec0801d25a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert continuous data into binary trend\n",
        "def convert_to_trend(y):\n",
        "    trend = np.diff(y.flatten())  # Calculate the difference between consecutive points\n",
        "    trend_binary = (trend > 0).astype(int)  # 1 for increase, 0 for decrease\n",
        "    return trend_binary\n",
        "\n",
        "# Convert y_true and y_pred into trend directions\n",
        "y_true_trend_phasic = convert_to_trend(y_true_phasic)\n",
        "y_pred_trend_phasic = convert_to_trend(y_pred_phasic)\n",
        "\n",
        "y_true_trend_tonic = convert_to_trend(y_true_tonic)\n",
        "y_pred_trend_tonic = convert_to_trend(y_pred_tonic)\n",
        "\n"
      ],
      "metadata": {
        "id": "w68v3EsT3QHG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score\n",
        "\n",
        "# Function to evaluate trend-based metrics\n",
        "def evaluate_trend(y_true_trend, y_pred_trend):\n",
        "    accuracy = accuracy_score(y_true_trend, y_pred_trend)\n",
        "    f1 = f1_score(y_true_trend, y_pred_trend, average='macro')\n",
        "    sensitivity = recall_score(y_true_trend, y_pred_trend, average='macro')\n",
        "    auroc = roc_auc_score(y_true_trend, y_pred_trend)\n",
        "\n",
        "    return {\"Accuracy\": accuracy, \"F1 Score\": f1, \"Sensitivity\": sensitivity, \"AUROC\": auroc}\n",
        "\n",
        "# Evaluate phasic component trend\n",
        "metrics_new_phasic_trend = evaluate_trend(y_true_trend_phasic, y_pred_trend_phasic)\n",
        "print(\"New Phasic Component Trend Metrics:\", metrics_new_phasic_trend)\n",
        "\n",
        "# Evaluate tonic component trend\n",
        "metrics_new_tonic_trend = evaluate_trend(y_true_trend_tonic, y_pred_trend_tonic)\n",
        "print(\"New Tonic Component Trend Metrics:\", metrics_new_tonic_trend)\n"
      ],
      "metadata": {
        "id": "DMxK1lA84MI3",
        "outputId": "ca4138d3-cf6e-47d2-a6ea-9810078b87f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Phasic Component Trend Metrics: {'Accuracy': 0.5080789946140036, 'F1 Score': 0.5080012894906512, 'Sensitivity': 0.509886275523391, 'AUROC': 0.509886275523391}\n",
            "New Tonic Component Trend Metrics: {'Accuracy': 0.718132854578097, 'F1 Score': 0.41797283176593525, 'Sensitivity': 0.3853564547206166, 'AUROC': 0.3853564547206166}\n"
          ]
        }
      ]
    }
  ]
}